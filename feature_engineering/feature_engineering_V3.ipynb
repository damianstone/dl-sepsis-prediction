{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_project_root(marker=\".gitignore\"):\n",
    "    \"\"\"\n",
    "    walk up from the current working directory until a directory containing the\n",
    "    specified marker (e.g., .gitignore) is found.\n",
    "    \"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project root marker '{marker}' not found starting from {current}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           HR      O2Sat       Temp         SBP        MAP        DBP  \\\n",
      "0  102.108491  91.419811  36.919203  128.165094  88.199717  67.007325   \n",
      "1   97.000000  95.000000  36.919203   98.000000  75.330000  67.007325   \n",
      "2   89.000000  99.000000  36.919203  122.000000  86.000000  67.007325   \n",
      "3   90.000000  95.000000  36.919203  122.000000  88.665000  67.007325   \n",
      "4  103.000000  88.500000  36.919203  122.000000  91.330000  67.007325   \n",
      "\n",
      "        Resp    EtCO2  BaseExcess       HCO3  ...    Age  Gender     Unit1  \\\n",
      "0  24.712264  29.6875    0.091837  22.811236  ...  83.14       0  0.755119   \n",
      "1  19.000000  29.6875    0.091837  22.811236  ...  83.14       0  0.755119   \n",
      "2  22.000000  29.6875    0.091837  22.811236  ...  83.14       0  0.755119   \n",
      "3  30.000000  29.6875   24.000000  22.811236  ...  83.14       0  0.755119   \n",
      "4  24.500000  29.6875    0.091837  22.811236  ...  83.14       0  0.755119   \n",
      "\n",
      "      Unit2  HospAdmTime  ICULOS  SepsisLabel  patient_id  dataset  \\\n",
      "0  0.244881        -0.03       1            0           1        A   \n",
      "1  0.244881        -0.03       2            0           1        A   \n",
      "2  0.244881        -0.03       3            0           1        A   \n",
      "3  0.244881        -0.03       4            0           1        A   \n",
      "4  0.244881        -0.03       5            0           1        A   \n",
      "\n",
      "      cluster_id  \n",
      "0  0_9_4_3_X_X_X  \n",
      "1  0_9_4_3_X_X_X  \n",
      "2  0_9_4_3_X_X_X  \n",
      "3  0_9_4_3_X_X_X  \n",
      "4  0_9_4_3_X_X_X  \n",
      "\n",
      "[5 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the old preprocess_V2 data\n",
    "root = find_project_root()\n",
    "\n",
    "df = pd.read_parquet(root / \"dataset\" / \"Fully_imputed_dataset.parquet\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# print the first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_linear_interpolation(data, column_name):\n",
    "#     \"\"\"\n",
    "#     Imputes missing values using linear interpolation.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data (pd.DataFrame): The data frame containing the column values.\n",
    "#     - column_name (str): The name of the column to impute.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: DataFrame with missing values imputed using linear interpolation.\n",
    "#     \"\"\"\n",
    "#     imputed_data = data\n",
    "#     imputed_data[column_name] = data[column_name].interpolate(method='linear')\n",
    "#     return imputed_data\n",
    "\n",
    "# columns_to_linearly_interpolate = [\"HR\", \"O2Sat\", \"SBP\", \"MAP\", \"DBP\", \"Resp\"]\n",
    "\n",
    "# # Linear Interpolation\n",
    "# print(\"Linearly interpolating:\")\n",
    "# for col in columns_to_linearly_interpolate:\n",
    "#     if col != \"SepsisLabel\":  # Ensure we do not interpolate 'SepsisLabel'\n",
    "#         df = impute_linear_interpolation(df, col)\n",
    "#         print(col)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sep_col = [\n",
    "    \"BaseExcess\",\n",
    "    \"HCO3\",\n",
    "    \"FiO2\",\n",
    "    \"pH\",\n",
    "    \"PaCO2\",\n",
    "    \"SaO2\",\n",
    "    \"AST\",\n",
    "    \"BUN\",\n",
    "    \"Alkalinephos\",\n",
    "    \"Calcium\",\n",
    "    \"Chloride\",\n",
    "    \"Creatinine\",\n",
    "    \"Glucose\",\n",
    "    \"Lactate\",\n",
    "    \"Magnesium\",\n",
    "    \"Phosphate\",\n",
    "    \"Potassium\",\n",
    "    \"Bilirubin_total\",\n",
    "    \"Hct\",\n",
    "    \"Hgb\",\n",
    "    \"PTT\",\n",
    "    \"WBC\",\n",
    "    \"Platelets\",\n",
    "    \"Bilirubin_direct\",\n",
    "    \"Fibrinogen\",\n",
    "]\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = [\"HR\", \"O2Sat\", \"Temp\", \"SBP\", \"MAP\", \"DBP\", \"Resp\", \"EtCO2\"]\n",
    "\n",
    "\n",
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "\n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if len(nan_pos) == 0:\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else:\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos) - 1):\n",
    "                interval_f1[nan_pos[p] : nan_pos[p + 1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[: nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q + 1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.nan\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[: nan_pos[1]] = np.nan\n",
    "            for p in range(1, len(nan_pos) - 1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p + 1]] = (\n",
    "                    data[nan_pos[p]] - data[nan_pos[p - 1]]\n",
    "                )\n",
    "            diff_f[nan_pos[-1] :] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def feature_slide_window(patient_data, columns):\n",
    "\n",
    "    window_size = 6\n",
    "    features = {}\n",
    "\n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f\"{column}_max\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).max()\n",
    "        features[f\"{column}_min\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).min()\n",
    "        features[f\"{column}_mean\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).mean()\n",
    "        features[f\"{column}_median\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).median()\n",
    "        features[f\"{column}_std\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).std()\n",
    "\n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f\"{column}_diff_std\"] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "\n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def extract_features(patient_data, columns_to_drop=[]):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data[\"SepsisLabel\"])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables\n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered\n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.ffill()\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "\n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = [\"HR\", \"O2Sat\", \"SBP\", \"MAP\", \"Resp\"]\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "\n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Data Pre-processing\n",
    "def preprecess_data(df, patient_id_map=None):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "    total_patients = len(set(df.index.get_level_values(0)))\n",
    "    i = 0\n",
    "    for patient_id in set(df.index.get_level_values(0)):\n",
    "        i += 1\n",
    "        print(f\"Processing patient {i} of {total_patients}\",  end=\"\\r\",)\n",
    "        if patient_id_map is not None:\n",
    "            print(\n",
    "                f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        patient_data = df.loc[patient_id]\n",
    "\n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "\n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "\n",
    "    return data_features, data_labels\n",
    "\n",
    "\n",
    "def preprecess_data2(df, patient_id_map=None):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "\n",
    "    for patient_id in set(df.index.get_level_values(0)):\n",
    "        if patient_id_map is not None:\n",
    "            print(\n",
    "                f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        patient_data = df.loc[patient_id]\n",
    "\n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features, index=[patient_id] * len(features))\n",
    "        labels = pd.DataFrame(labels, index=[patient_id] * len(labels))\n",
    "\n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = pd.concat(frames_features)\n",
    "    data_labels = pd.concat(frames_labels)\n",
    "\n",
    "    return data_features, data_labels.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_nan_indicators(df):\n",
    "    df = df.copy()\n",
    "    for column in df.columns:\n",
    "        df[column + \"_nan\"] = df[column].isna().astype(int)\n",
    "    df = df.copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set patient id as index\n",
    "df.set_index(\"patient_id\", inplace=True)\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess data\n",
      "Processing patient 40336 of 40336\r"
     ]
    }
   ],
   "source": [
    "\n",
    "X = add_nan_indicators(df)\n",
    "print(\"preprocess data\")\n",
    "XX, y = preprecess_data(X)\n",
    "new_feature_names = [f\"new_feature_{i}\" for i in range(XX.shape[1])]\n",
    "XX_df = pd.DataFrame(XX, columns=new_feature_names, index=X.index)\n",
    "\n",
    "# Concatenate the new features from XX_df back to the original DataFrame X\n",
    "X = pd.concat([X, XX_df], axis=1)\n",
    "y = df[\"SepsisLabel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains NaN or infinite values. Handling...\n"
     ]
    }
   ],
   "source": [
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    # Replace infinite values with NaN so they can be filled too\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # First apply forward fill\n",
    "    X.fillna(method=\"ffill\", inplace=True)\n",
    "    # Then apply backward fill for any remaining NaNs\n",
    "    X.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552210, 309)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>...</th>\n",
       "      <th>new_feature_213</th>\n",
       "      <th>new_feature_214</th>\n",
       "      <th>new_feature_215</th>\n",
       "      <th>new_feature_216</th>\n",
       "      <th>new_feature_217</th>\n",
       "      <th>new_feature_218</th>\n",
       "      <th>new_feature_219</th>\n",
       "      <th>new_feature_220</th>\n",
       "      <th>new_feature_221</th>\n",
       "      <th>new_feature_222</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102.108491</td>\n",
       "      <td>91.419811</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>128.165094</td>\n",
       "      <td>88.199717</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>24.712264</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>4.604346</td>\n",
       "      <td>7.284687</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>75.330000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>10.357035</td>\n",
       "      <td>7.284687</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>6.575840</td>\n",
       "      <td>9.241212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>88.665000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.848754</td>\n",
       "      <td>5.778552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>91.330000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>6.058052</td>\n",
       "      <td>3.292669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 310 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id          HR      O2Sat       Temp         SBP        MAP  \\\n",
       "0           1  102.108491  91.419811  36.919203  128.165094  88.199717   \n",
       "1           1   97.000000  95.000000  36.919203   98.000000  75.330000   \n",
       "2           1   89.000000  99.000000  36.919203  122.000000  86.000000   \n",
       "3           1   90.000000  95.000000  36.919203  122.000000  88.665000   \n",
       "4           1  103.000000  88.500000  36.919203  122.000000  91.330000   \n",
       "\n",
       "         DBP       Resp    EtCO2  BaseExcess  ...  new_feature_213  \\\n",
       "0  67.007325  24.712264  29.6875    0.091837  ...         4.604346   \n",
       "1  67.007325  19.000000  29.6875    0.091837  ...        10.357035   \n",
       "2  67.007325  22.000000  29.6875    0.091837  ...         6.575840   \n",
       "3  67.007325  30.000000  29.6875   24.000000  ...         4.848754   \n",
       "4  67.007325  24.500000  29.6875    0.091837  ...         6.058052   \n",
       "\n",
       "   new_feature_214  new_feature_215  new_feature_216  new_feature_217  \\\n",
       "0         7.284687              2.0              0.0              0.0   \n",
       "1         7.284687              3.0              0.0              3.0   \n",
       "2         9.241212              0.0              0.0              2.0   \n",
       "3         5.778552              1.0              0.0              3.0   \n",
       "4         3.292669              0.0              0.0              3.0   \n",
       "\n",
       "   new_feature_218  new_feature_219  new_feature_220  new_feature_221  \\\n",
       "0              0.0              0.0              0.0              3.0   \n",
       "1              1.0              0.0              0.0              3.0   \n",
       "2              1.0              0.0              0.0              3.0   \n",
       "3              0.0              1.0              1.0              3.0   \n",
       "4              1.0              0.0              0.0              3.0   \n",
       "\n",
       "   new_feature_222  \n",
       "0              2.0  \n",
       "1              2.0  \n",
       "2              2.0  \n",
       "3              2.0  \n",
       "4              2.0  \n",
       "\n",
       "[5 rows x 310 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prnint the number of columns in X\n",
    "print(X.shape)\n",
    "\n",
    "# concat X and y\n",
    "X[\"SepsisLabel\"] = y\n",
    "\n",
    "# patient id is the index make it into a normal column\n",
    "X = X.reset_index(drop=False)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP USELESS COLUMNS DONE\n",
      "HANDLE GENDER DONE\n"
     ]
    }
   ],
   "source": [
    "X = X.drop(columns=[\"Unit1\", \"Unit2\", \"cluster_id\", \"dataset\"])\n",
    "print(\"DROP USELESS COLUMNS DONE\")\n",
    "# handle gender as a categorical variable\n",
    "X[\"Gender\"] = LabelEncoder().fit_transform(\n",
    "    X[\"Gender\"].astype(str)\n",
    ")\n",
    "print(\"HANDLE GENDER DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert dtypes to strings\n",
    "dtypes = {col: str(dtype) for col, dtype in X.dtypes.to_dict().items()}\n",
    "\n",
    "# Save to JSON\n",
    "with open(root / \"dataset\" / \"dtypes.json\", \"w\") as f:\n",
    "    json.dump(dtypes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'B']\n",
      "['0_7_4_3_X_X_X' '1_0_4_0_X_X_X' '1_3_0_1_X_X_X' '0_2_4_0_X_X_X'\n",
      " '0_7_1_4_X_X_X' '1_2_1_4_X_X_X' '0_8_2_1_X_X_X' '0_8_1_0_X_X_X'\n",
      " '0_1_4_3_X_X_X' '1_1_0_4_X_X_X' '0_6_3_3_X_X_X' '1_8_3_0_X_X_X'\n",
      " '0_7_4_1_X_X_X' '0_6_3_0_X_X_X' '1_1_4_4_X_X_X' '1_8_2_2_X_X_X'\n",
      " '1_8_3_1_X_X_X' '0_6_4_3_X_X_X' '0_4_3_0_X_X_X' '1_2_1_0_X_X_X'\n",
      " '0_6_0_0_X_X_X' '0_7_4_0_X_X_X' '0_5_2_1_X_X_X' '1_3_2_2_X_X_X'\n",
      " '0_2_1_3_X_X_X' '1_9_1_0_X_X_X' '0_0_1_3_X_X_X' '1_8_3_4_X_X_X'\n",
      " '1_3_3_3_X_X_X' '1_5_3_4_X_X_X' '1_3_2_1_X_X_X' '1_4_1_2_X_X_X'\n",
      " '0_8_2_3_X_X_X' '0_5_3_0_X_X_X' '1_5_3_3_X_X_X' '1_1_0_3_X_X_X'\n",
      " '0_7_4_4_X_X_X' '1_9_0_3_X_X_X' '1_4_3_0_X_X_X' '1_5_4_3_X_X_X'\n",
      " '1_7_3_2_X_X_X' '0_3_1_2_X_X_X' '1_3_2_4_X_X_X' '0_9_1_0_X_X_X'\n",
      " '0_2_4_2_X_X_X' '0_7_1_3_X_X_X' '1_5_4_4_X_X_X' '0_9_2_1_X_X_X'\n",
      " '1_4_4_4_X_X_X' '0_0_4_3_X_X_X' '1_8_2_0_X_X_X' '0_8_4_0_X_X_X'\n",
      " '0_7_2_1_X_X_X' '1_6_3_2_X_X_X' '1_2_1_3_X_X_X' '1_4_4_1_X_X_X'\n",
      " '1_1_3_4_X_X_X' '0_1_3_1_X_X_X' '1_2_0_3_X_X_X' '0_2_1_1_X_X_X'\n",
      " '1_7_1_0_X_X_X' '0_5_1_0_X_X_X' '0_2_4_3_X_X_X' '1_4_4_3_X_X_X'\n",
      " '1_6_1_2_X_X_X' '1_8_4_1_X_X_X' '0_7_3_2_X_X_X' '1_3_4_4_X_X_X'\n",
      " '1_3_3_2_X_X_X' '1_5_0_3_X_X_X' '1_4_0_0_X_X_X' '1_2_4_3_X_X_X'\n",
      " '1_2_2_0_X_X_X' '1_3_1_1_X_X_X' '1_7_2_1_X_X_X' '0_9_1_3_X_X_X'\n",
      " '1_3_0_2_X_X_X' '0_4_1_1_X_X_X' '1_3_2_3_X_X_X' '1_1_2_2_X_X_X'\n",
      " '0_7_0_2_X_X_X' '1_0_1_0_X_X_X' '1_4_0_4_X_X_X' '0_1_4_1_X_X_X'\n",
      " '0_3_1_4_X_X_X' '1_3_4_1_X_X_X' '1_7_0_3_X_X_X' '0_2_0_3_X_X_X'\n",
      " '1_7_1_4_X_X_X' '1_4_0_1_X_X_X' '1_7_0_2_X_X_X' '1_8_3_3_X_X_X'\n",
      " '1_8_1_4_X_X_X' '1_3_4_0_X_X_X' '0_5_1_4_X_X_X' '0_7_2_3_X_X_X'\n",
      " '0_1_2_4_X_X_X' '0_5_2_3_X_X_X' '1_4_2_1_X_X_X' '1_8_0_3_X_X_X'\n",
      " '1_6_0_4_X_X_X' '0_1_3_3_X_X_X' '1_0_1_3_X_X_X' '1_5_0_1_X_X_X'\n",
      " '0_2_3_3_X_X_X' '1_1_4_3_X_X_X' '1_6_4_0_X_X_X' '0_0_0_1_X_X_X'\n",
      " '1_7_4_2_X_X_X' '0_4_3_2_X_X_X' '0_5_4_2_X_X_X' '0_4_4_4_X_X_X'\n",
      " '1_3_4_3_X_X_X' '0_7_0_3_X_X_X' '0_5_0_1_X_X_X' '0_0_4_2_X_X_X'\n",
      " '1_5_3_0_X_X_X' '1_9_2_0_X_X_X' '1_0_0_2_X_X_X' '0_5_4_1_X_X_X'\n",
      " '0_0_0_3_X_X_X' '0_0_3_2_X_X_X' '0_0_3_3_X_X_X' '0_0_4_1_X_X_X'\n",
      " '1_0_2_3_X_X_X' '0_8_0_3_X_X_X' '1_6_2_3_X_X_X' '1_5_2_1_X_X_X'\n",
      " '0_9_0_0_X_X_X' '1_1_0_0_X_X_X' '0_3_2_4_X_X_X' '1_6_4_2_X_X_X'\n",
      " '0_6_1_2_X_X_X' '1_2_3_2_X_X_X' '1_9_4_1_X_X_X' '1_8_4_0_X_X_X'\n",
      " '0_5_4_3_X_X_X' '1_2_0_1_X_X_X' '1_3_0_0_X_X_X' '1_4_2_0_X_X_X'\n",
      " '0_1_2_0_X_X_X' '0_5_4_4_X_X_X' '1_1_2_4_X_X_X' '0_3_2_3_X_X_X'\n",
      " '0_3_3_1_X_X_X' '1_2_1_2_X_X_X' '0_3_3_4_X_X_X' '0_6_2_4_X_X_X'\n",
      " '1_3_1_3_X_X_X' '1_7_4_3_X_X_X' '1_6_2_2_X_X_X' '1_9_4_3_X_X_X'\n",
      " '1_5_1_4_X_X_X' '1_9_1_3_X_X_X' '1_9_0_0_X_X_X' '0_1_1_4_X_X_X'\n",
      " '1_1_4_2_X_X_X' '1_9_4_0_X_X_X' '0_3_4_4_X_X_X' '0_2_3_0_X_X_X'\n",
      " '1_7_3_1_X_X_X' '1_0_2_1_X_X_X' '0_1_2_2_X_X_X' '0_8_0_0_X_X_X'\n",
      " '1_9_0_1_X_X_X' '1_2_0_4_X_X_X' '1_7_2_0_X_X_X' '1_1_3_3_X_X_X'\n",
      " '0_9_4_4_X_X_X' '1_2_4_0_X_X_X' '0_2_0_2_X_X_X' '1_6_1_0_X_X_X'\n",
      " '0_1_1_2_X_X_X' '1_3_3_0_X_X_X' '1_7_1_1_X_X_X' '1_9_2_3_X_X_X'\n",
      " '0_6_2_1_X_X_X' '0_3_3_3_X_X_X' '1_0_4_2_X_X_X' '0_4_3_3_X_X_X'\n",
      " '0_6_0_4_X_X_X' '0_8_3_0_X_X_X' '1_3_1_0_X_X_X' '1_1_0_2_X_X_X'\n",
      " '1_7_0_0_X_X_X' '1_9_3_2_X_X_X' '1_8_0_1_X_X_X' '1_4_2_2_X_X_X'\n",
      " '1_1_3_2_X_X_X' '1_0_1_1_X_X_X' '1_1_1_4_X_X_X' '0_6_2_2_X_X_X'\n",
      " '1_7_0_4_X_X_X' '0_9_3_0_X_X_X' '1_6_4_4_X_X_X' '1_8_1_3_X_X_X'\n",
      " '1_3_1_4_X_X_X' '1_9_3_1_X_X_X' '0_1_3_2_X_X_X' '0_8_1_3_X_X_X'\n",
      " '0_1_1_3_X_X_X' '0_1_0_3_X_X_X' '1_1_2_3_X_X_X' '1_2_4_2_X_X_X'\n",
      " '1_6_0_3_X_X_X' '0_9_0_4_X_X_X' '1_8_2_4_X_X_X' '1_0_4_4_X_X_X'\n",
      " '1_6_0_1_X_X_X' '1_8_0_4_X_X_X' '0_8_1_1_X_X_X' '0_7_0_1_X_X_X'\n",
      " '1_8_4_4_X_X_X' '1_9_2_1_X_X_X' '0_9_3_1_X_X_X' '1_5_1_1_X_X_X'\n",
      " '1_8_1_2_X_X_X' '0_9_0_1_X_X_X' '0_2_1_0_X_X_X' '0_6_4_1_X_X_X'\n",
      " '0_3_1_1_X_X_X' '1_1_1_3_X_X_X' '1_6_2_4_X_X_X' '0_9_0_2_X_X_X'\n",
      " '1_5_2_3_X_X_X' '0_8_2_2_X_X_X' '0_3_3_2_X_X_X' '0_8_3_2_X_X_X'\n",
      " '1_9_1_1_X_X_X' '1_9_0_2_X_X_X' '0_6_1_1_X_X_X' '0_8_1_4_X_X_X'\n",
      " '1_5_0_2_X_X_X' '1_1_4_1_X_X_X' '0_9_1_1_X_X_X' '1_7_3_0_X_X_X'\n",
      " '0_3_4_2_X_X_X' '0_2_4_4_X_X_X' '0_8_4_2_X_X_X' '0_2_1_2_X_X_X'\n",
      " '0_8_1_2_X_X_X' '0_5_3_4_X_X_X' '1_7_1_2_X_X_X' '1_5_1_0_X_X_X'\n",
      " '1_3_3_1_X_X_X' '1_1_1_0_X_X_X' '0_8_0_1_X_X_X' '0_3_2_1_X_X_X'\n",
      " '0_9_2_0_X_X_X' '0_4_3_4_X_X_X' '1_3_1_2_X_X_X' '0_1_4_4_X_X_X'\n",
      " '0_5_0_3_X_X_X' '0_0_0_2_X_X_X' '1_4_0_3_X_X_X' '0_0_4_4_X_X_X'\n",
      " '1_8_1_0_X_X_X' '0_7_3_3_X_X_X' '1_0_4_3_X_X_X' '0_2_0_1_X_X_X'\n",
      " '0_9_4_0_X_X_X' '1_0_2_2_X_X_X' '1_9_1_2_X_X_X' '1_3_4_2_X_X_X'\n",
      " '1_4_1_0_X_X_X' '0_0_1_4_X_X_X' '1_9_3_0_X_X_X' '0_0_2_1_X_X_X'\n",
      " '0_4_2_1_X_X_X' '1_2_4_4_X_X_X' '1_6_3_1_X_X_X' '1_2_3_1_X_X_X'\n",
      " '1_5_0_0_X_X_X' '0_4_0_2_X_X_X' '1_7_2_2_X_X_X' '0_4_4_1_X_X_X'\n",
      " '1_3_0_4_X_X_X' '1_4_1_3_X_X_X' '1_5_4_1_X_X_X' '1_0_3_2_X_X_X'\n",
      " '1_4_3_3_X_X_X' '1_7_3_4_X_X_X' '1_4_3_4_X_X_X' '1_0_2_4_X_X_X'\n",
      " '1_6_4_1_X_X_X' '1_8_3_2_X_X_X' '1_2_2_4_X_X_X' '0_7_2_4_X_X_X'\n",
      " '1_8_4_3_X_X_X' '0_5_0_0_X_X_X' '1_7_4_0_X_X_X' '0_9_3_4_X_X_X'\n",
      " '1_1_0_1_X_X_X' '1_6_3_0_X_X_X' '1_5_2_0_X_X_X' '0_7_3_0_X_X_X'\n",
      " '1_1_1_1_X_X_X' '0_2_2_0_X_X_X' '0_6_2_0_X_X_X' '1_2_3_4_X_X_X'\n",
      " '0_3_0_2_X_X_X' '1_5_4_0_X_X_X' '0_0_3_0_X_X_X' '0_1_4_0_X_X_X'\n",
      " '0_2_1_4_X_X_X' '0_7_0_4_X_X_X' '0_1_1_0_X_X_X' '0_9_2_4_X_X_X'\n",
      " '1_6_4_3_X_X_X' '1_2_2_3_X_X_X' '0_4_1_4_X_X_X' '1_0_3_1_X_X_X'\n",
      " '1_0_0_4_X_X_X' '1_6_3_3_X_X_X' '0_2_0_4_X_X_X' '1_4_1_4_X_X_X'\n",
      " '0_6_4_4_X_X_X' '0_6_2_3_X_X_X' '1_6_0_0_X_X_X' '1_8_0_2_X_X_X'\n",
      " '0_6_4_0_X_X_X' '1_2_3_0_X_X_X' '1_7_2_4_X_X_X' '1_5_1_2_X_X_X'\n",
      " '1_1_2_1_X_X_X' '0_8_3_3_X_X_X' '1_8_1_1_X_X_X' '1_5_4_2_X_X_X'\n",
      " '0_3_2_0_X_X_X' '0_9_1_4_X_X_X' '0_9_1_2_X_X_X' '1_2_3_3_X_X_X'\n",
      " '0_1_0_2_X_X_X' '0_3_4_3_X_X_X' '0_4_2_2_X_X_X' '1_1_4_0_X_X_X'\n",
      " '0_0_2_0_X_X_X' '0_1_0_1_X_X_X' '1_6_2_1_X_X_X' '0_1_1_1_X_X_X'\n",
      " '1_1_3_1_X_X_X' '0_5_2_2_X_X_X' '0_4_4_0_X_X_X' '0_7_1_0_X_X_X'\n",
      " '0_2_2_4_X_X_X' '1_5_2_2_X_X_X' '1_0_0_1_X_X_X' '0_6_3_4_X_X_X'\n",
      " '0_4_2_3_X_X_X' '0_0_4_0_X_X_X' '1_6_2_0_X_X_X' '0_7_1_1_X_X_X'\n",
      " '1_8_4_2_X_X_X' '0_4_3_1_X_X_X' '0_3_1_0_X_X_X' '1_5_0_4_X_X_X'\n",
      " '1_9_1_4_X_X_X' '1_1_3_0_X_X_X' '0_9_4_2_X_X_X' '0_8_0_4_X_X_X'\n",
      " '0_3_4_1_X_X_X' '0_8_3_4_X_X_X' '1_3_3_4_X_X_X' '0_0_0_4_X_X_X'\n",
      " '1_4_3_1_X_X_X' '0_5_4_0_X_X_X' '0_5_0_2_X_X_X' '0_8_2_0_X_X_X'\n",
      " '0_4_0_0_X_X_X' '0_0_3_1_X_X_X' '0_4_4_3_X_X_X' '1_0_1_2_X_X_X'\n",
      " '1_0_3_3_X_X_X' '1_1_2_0_X_X_X' '0_9_4_1_X_X_X' '0_9_3_2_X_X_X'\n",
      " '0_7_1_2_X_X_X' '0_4_1_0_X_X_X' '1_5_3_2_X_X_X' '1_7_2_3_X_X_X'\n",
      " '0_7_0_0_X_X_X' '0_8_0_2_X_X_X' '0_4_0_3_X_X_X' '0_5_3_2_X_X_X'\n",
      " '1_2_4_1_X_X_X' '1_4_4_2_X_X_X' '0_4_4_2_X_X_X' '1_0_3_4_X_X_X'\n",
      " '0_6_0_3_X_X_X' '0_2_3_2_X_X_X' '0_9_4_3_X_X_X' '1_1_1_2_X_X_X'\n",
      " '0_4_1_3_X_X_X' '1_3_0_3_X_X_X' '1_0_1_4_X_X_X' '0_9_0_3_X_X_X'\n",
      " '0_8_4_1_X_X_X' '1_5_2_4_X_X_X' '1_8_0_0_X_X_X' '1_4_0_2_X_X_X'\n",
      " '1_4_3_2_X_X_X' '0_0_3_4_X_X_X' '1_4_2_4_X_X_X' '1_2_2_1_X_X_X'\n",
      " '0_9_2_2_X_X_X' '1_7_4_4_X_X_X' '0_2_2_1_X_X_X' '0_2_3_4_X_X_X'\n",
      " '0_5_3_3_X_X_X' '0_2_2_3_X_X_X' '0_7_4_2_X_X_X' '1_6_0_2_X_X_X'\n",
      " '0_1_3_4_X_X_X' '0_2_0_0_X_X_X' '0_5_3_1_X_X_X' '0_1_0_4_X_X_X'\n",
      " '0_8_2_4_X_X_X' '0_6_0_2_X_X_X' '1_2_0_0_X_X_X' '0_6_1_0_X_X_X'\n",
      " '1_6_1_1_X_X_X' '1_2_2_2_X_X_X' '0_1_2_3_X_X_X' '0_4_0_4_X_X_X'\n",
      " '0_9_2_3_X_X_X' '0_4_2_4_X_X_X' '0_0_1_0_X_X_X' '0_5_2_0_X_X_X'\n",
      " '0_4_2_0_X_X_X' '1_5_3_1_X_X_X' '1_7_3_3_X_X_X' '1_5_1_3_X_X_X'\n",
      " '0_0_2_3_X_X_X' '0_9_3_3_X_X_X' '1_2_0_2_X_X_X' '0_3_1_3_X_X_X'\n",
      " '0_6_1_3_X_X_X' '0_0_0_0_X_X_X' '1_7_1_3_X_X_X' '1_9_2_2_X_X_X'\n",
      " '0_8_4_4_X_X_X' '0_6_1_4_X_X_X' '1_7_0_1_X_X_X' '1_9_3_3_X_X_X'\n",
      " '0_3_0_3_X_X_X' '0_8_3_1_X_X_X' '0_4_1_2_X_X_X' '1_9_2_4_X_X_X'\n",
      " '0_0_2_2_X_X_X' '0_3_0_4_X_X_X' '1_6_1_3_X_X_X' '1_8_2_1_X_X_X'\n",
      " '1_3_2_0_X_X_X' '0_5_2_4_X_X_X' '1_8_2_3_X_X_X' '0_2_4_1_X_X_X'\n",
      " '0_5_0_4_X_X_X' '0_6_0_1_X_X_X' '0_1_4_2_X_X_X' '0_3_0_1_X_X_X'\n",
      " '1_4_4_0_X_X_X' '0_6_3_1_X_X_X' '0_5_1_2_X_X_X' '1_9_4_2_X_X_X'\n",
      " '0_7_2_0_X_X_X' '1_4_1_1_X_X_X' '1_9_0_4_X_X_X' '0_3_3_0_X_X_X'\n",
      " '1_0_0_3_X_X_X' '0_7_2_2_X_X_X' '1_0_4_1_X_X_X' '0_7_3_1_X_X_X'\n",
      " '0_8_4_3_X_X_X' '0_0_1_2_X_X_X' '0_0_1_1_X_X_X' '0_5_1_3_X_X_X'\n",
      " '0_1_2_1_X_X_X' '0_2_3_1_X_X_X' '1_4_2_3_X_X_X' '0_3_2_2_X_X_X'\n",
      " '1_6_1_4_X_X_X' '0_7_3_4_X_X_X' '1_0_2_0_X_X_X' '0_3_4_0_X_X_X'\n",
      " '0_2_2_2_X_X_X' '1_2_1_1_X_X_X' '0_6_4_2_X_X_X' '1_6_3_4_X_X_X'\n",
      " '0_1_0_0_X_X_X' '1_9_3_4_X_X_X' '0_5_1_1_X_X_X' '0_6_3_2_X_X_X'\n",
      " '1_9_4_4_X_X_X' '1_0_3_0_X_X_X' '1_7_4_1_X_X_X' '0_3_0_0_X_X_X'\n",
      " '0_0_2_4_X_X_X' '0_4_0_1_X_X_X' '0_1_3_0_X_X_X' '1_0_0_0_X_X_X']\n"
     ]
    }
   ],
   "source": [
    "print(X['new_feature_41'].unique())\n",
    "print(X['new_feature_42'].unique())\n",
    "#drop the new_feature_41 and new_feature_42 columns\n",
    "X = X.drop(columns=[\"new_feature_41\", \"new_feature_42\"])\n",
    "# save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "X.to_parquet(root / \"dataset\" / \"V35_preprocessed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
