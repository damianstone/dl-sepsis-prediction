{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_project_root(marker=\".gitignore\"):\n",
    "    \"\"\"\n",
    "    walk up from the current working directory until a directory containing the\n",
    "    specified marker (e.g., .gitignore) is found.\n",
    "    \"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project root marker '{marker}' not found starting from {current}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           HR      O2Sat       Temp         SBP        MAP        DBP  \\\n",
      "0  102.108491  91.419811  36.919203  128.165094  88.199717  67.007325   \n",
      "1   97.000000  95.000000  36.919203   98.000000  75.330000  67.007325   \n",
      "2   89.000000  99.000000  36.919203  122.000000  86.000000  67.007325   \n",
      "3   90.000000  95.000000  36.919203  122.000000  88.665000  67.007325   \n",
      "4  103.000000  88.500000  36.919203  122.000000  91.330000  67.007325   \n",
      "\n",
      "        Resp    EtCO2  BaseExcess       HCO3  ...  MAP_mean_6h  MAP_median_6h  \\\n",
      "0  24.712264  29.6875    0.091837  22.811236  ...    88.199717      88.199717   \n",
      "1  19.000000  29.6875    0.091837  22.811236  ...    81.764858      81.764858   \n",
      "2  22.000000  29.6875    0.091837  22.811236  ...    83.176572      86.000000   \n",
      "3  30.000000  29.6875   24.000000  22.811236  ...    84.548679      87.099858   \n",
      "4  24.500000  29.6875    0.091837  22.811236  ...    85.904943      88.199717   \n",
      "\n",
      "   MAP_std_6h  MAP_diff_std_6h  Resp_max_6h  Resp_min_6h  Resp_mean_6h  \\\n",
      "0    9.100264        16.645094    24.712264    24.712264     24.712264   \n",
      "1    9.100264        16.645094    24.712264    19.000000     21.856132   \n",
      "2    6.883764        16.645094    24.712264    19.000000     21.904088   \n",
      "3    6.254720        11.968888    30.000000    19.000000     23.928066   \n",
      "4    6.207930         9.852805    30.000000    19.000000     24.042453   \n",
      "\n",
      "   Resp_median_6h  Resp_std_6h  Resp_diff_std_6h  \n",
      "0       24.712264     4.039181          6.160501  \n",
      "1       21.856132     4.039181          6.160501  \n",
      "2       22.000000     2.857340          6.160501  \n",
      "3       23.356132     4.672138          6.939377  \n",
      "4       24.500000     4.054267          6.729752  \n",
      "\n",
      "[5 rows x 109 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the old preprocess_V2 data\n",
    "root = find_project_root()\n",
    "\n",
    "df = pd.read_parquet(root / \"dataset\" / \"V2_preprocessed.parquet\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "# print the first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sep_col = [\n",
    "    \"BaseExcess\",\n",
    "    \"HCO3\",\n",
    "    \"FiO2\",\n",
    "    \"pH\",\n",
    "    \"PaCO2\",\n",
    "    \"SaO2\",\n",
    "    \"AST\",\n",
    "    \"BUN\",\n",
    "    \"Alkalinephos\",\n",
    "    \"Calcium\",\n",
    "    \"Chloride\",\n",
    "    \"Creatinine\",\n",
    "    \"Glucose\",\n",
    "    \"Lactate\",\n",
    "    \"Magnesium\",\n",
    "    \"Phosphate\",\n",
    "    \"Potassium\",\n",
    "    \"Bilirubin_total\",\n",
    "    \"Hct\",\n",
    "    \"Hgb\",\n",
    "    \"PTT\",\n",
    "    \"WBC\",\n",
    "    \"Platelets\",\n",
    "    \"Bilirubin_direct\",\n",
    "    \"Fibrinogen\",\n",
    "]\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = [\"HR\", \"O2Sat\", \"Temp\", \"SBP\", \"MAP\", \"DBP\", \"Resp\", \"EtCO2\"]\n",
    "\n",
    "\n",
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "\n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if len(nan_pos) == 0:\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else:\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos) - 1):\n",
    "                interval_f1[nan_pos[p] : nan_pos[p + 1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[: nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q + 1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.nan\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[: nan_pos[1]] = np.nan\n",
    "            for p in range(1, len(nan_pos) - 1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p + 1]] = (\n",
    "                    data[nan_pos[p]] - data[nan_pos[p - 1]]\n",
    "                )\n",
    "            diff_f[nan_pos[-1] :] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def feature_slide_window(patient_data, columns):\n",
    "\n",
    "    window_size = 6\n",
    "    features = {}\n",
    "\n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f\"{column}_max\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).max()\n",
    "        features[f\"{column}_min\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).min()\n",
    "        features[f\"{column}_mean\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).mean()\n",
    "        features[f\"{column}_median\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).median()\n",
    "        features[f\"{column}_std\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).std()\n",
    "\n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f\"{column}_diff_std\"] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "\n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def extract_features(patient_data, columns_to_drop=[]):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data[\"SepsisLabel\"])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables\n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered\n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.ffill()\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "\n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = [\"HR\", \"O2Sat\", \"SBP\", \"MAP\", \"Resp\"]\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "\n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Data Pre-processing\n",
    "def preprecess_data(dataset, patient_id_map=None):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "    total_patients = len(set(dataset.index.get_level_values(0)))\n",
    "    i = 0\n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        i += 1\n",
    "        print(f\"Processing patient {i} of {total_patients}\",  end=\"\\r\",)\n",
    "        if patient_id_map is not None:\n",
    "            print(\n",
    "                f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        patient_data = dataset.loc[patient_id]\n",
    "\n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "\n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "\n",
    "    return data_features, data_labels\n",
    "\n",
    "\n",
    "def preprecess_data2(dataset, patient_id_map=None):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "\n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        if patient_id_map is not None:\n",
    "            print(\n",
    "                f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        patient_data = dataset.loc[patient_id]\n",
    "\n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features, index=[patient_id] * len(features))\n",
    "        labels = pd.DataFrame(labels, index=[patient_id] * len(labels))\n",
    "\n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = pd.concat(frames_features)\n",
    "    data_labels = pd.concat(frames_labels)\n",
    "\n",
    "    return data_features, data_labels.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_nan_indicators(df):\n",
    "    df = df.copy()\n",
    "    for column in df.columns:\n",
    "        df[column + \"_nan\"] = df[column].isna().astype(int)\n",
    "    df = df.copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set patient id as index\n",
    "df.set_index(\"patient_id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/313642693.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column + \"_nan\"] = df[column].isna().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess data\n",
      "Processing patient 40336 of 40336\r"
     ]
    }
   ],
   "source": [
    "dataset = df.copy()\n",
    "X = add_nan_indicators(dataset)\n",
    "print(\"preprocess data\")\n",
    "XX, y = preprecess_data(X)\n",
    "new_feature_names = [f\"new_feature_{i}\" for i in range(XX.shape[1])]\n",
    "XX_df = pd.DataFrame(XX, columns=new_feature_names, index=X.index)\n",
    "\n",
    "# Concatenate the new features from XX_df back to the original DataFrame X\n",
    "X = pd.concat([X, XX_df], axis=1)\n",
    "y = dataset[\"SepsisLabel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contains NaN or infinite values. Handling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/4191023736.py:7: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X.fillna(method=\"ffill\", inplace=True)\n",
      "/var/folders/s0/ldb10jg949ndmfwfkbsz5x480000gn/T/ipykernel_84965/4191023736.py:9: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  X.fillna(method=\"bfill\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    # Replace infinite values with NaN so they can be filled too\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # First apply forward fill\n",
    "    X.fillna(method=\"ffill\", inplace=True)\n",
    "    # Then apply backward fill for any remaining NaNs\n",
    "    X.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method=\"ffill\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552210, 569)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>...</th>\n",
       "      <th>new_feature_343</th>\n",
       "      <th>new_feature_344</th>\n",
       "      <th>new_feature_345</th>\n",
       "      <th>new_feature_346</th>\n",
       "      <th>new_feature_347</th>\n",
       "      <th>new_feature_348</th>\n",
       "      <th>new_feature_349</th>\n",
       "      <th>new_feature_350</th>\n",
       "      <th>new_feature_351</th>\n",
       "      <th>new_feature_352</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102.108491</td>\n",
       "      <td>91.419811</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>128.165094</td>\n",
       "      <td>88.199717</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>24.712264</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1.169045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>75.330000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>1.661554</td>\n",
       "      <td>2.255992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>2.498333</td>\n",
       "      <td>3.895510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>88.665000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987421</td>\n",
       "      <td>1.114301</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>36.919203</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>91.330000</td>\n",
       "      <td>67.007325</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>29.6875</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>...</td>\n",
       "      <td>3.141125</td>\n",
       "      <td>3.600926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 570 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id          HR      O2Sat       Temp         SBP        MAP  \\\n",
       "0           1  102.108491  91.419811  36.919203  128.165094  88.199717   \n",
       "1           1   97.000000  95.000000  36.919203   98.000000  75.330000   \n",
       "2           1   89.000000  99.000000  36.919203  122.000000  86.000000   \n",
       "3           1   90.000000  95.000000  36.919203  122.000000  88.665000   \n",
       "4           1  103.000000  88.500000  36.919203  122.000000  91.330000   \n",
       "\n",
       "         DBP       Resp    EtCO2  BaseExcess  ...  new_feature_343  \\\n",
       "0  67.007325  24.712264  29.6875    0.091837  ...         0.816497   \n",
       "1  67.007325  19.000000  29.6875    0.091837  ...         1.661554   \n",
       "2  67.007325  22.000000  29.6875    0.091837  ...         2.498333   \n",
       "3  67.007325  30.000000  29.6875   24.000000  ...         0.987421   \n",
       "4  67.007325  24.500000  29.6875    0.091837  ...         3.141125   \n",
       "\n",
       "   new_feature_344  new_feature_345  new_feature_346  new_feature_347  \\\n",
       "0         1.169045              0.0              0.0              0.0   \n",
       "1         2.255992              0.0              0.0              0.0   \n",
       "2         3.895510              1.0              0.0              0.0   \n",
       "3         1.114301              1.0              0.0              2.0   \n",
       "4         3.600926              0.0              0.0              0.0   \n",
       "\n",
       "   new_feature_348  new_feature_349  new_feature_350  new_feature_351  \\\n",
       "0              1.0              0.0              0.0              3.0   \n",
       "1              1.0              0.0              0.0              3.0   \n",
       "2              1.0              0.0              0.0              3.0   \n",
       "3              1.0              0.0              0.0              3.0   \n",
       "4              2.0              0.0              0.0              2.0   \n",
       "\n",
       "   new_feature_352  \n",
       "0              2.0  \n",
       "1              2.0  \n",
       "2              2.0  \n",
       "3              2.0  \n",
       "4              2.0  \n",
       "\n",
       "[5 rows x 570 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prnint the number of columns in X\n",
    "print(X.shape)\n",
    "\n",
    "# concat X and y\n",
    "X[\"SepsisLabel\"] = y\n",
    "\n",
    "# patient id is the index make it into a normal column\n",
    "X = X.reset_index(drop=False)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "X.to_parquet(root / \"dataset\" / \"V3_preprocessed.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
