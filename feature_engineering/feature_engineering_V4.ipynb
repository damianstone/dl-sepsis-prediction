{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_project_root(marker=\".gitignore\"):\n",
    "    \"\"\"\n",
    "    walk up from the current working directory until a directory containing the\n",
    "    specified marker (e.g., .gitignore) is found.\n",
    "    \"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project root marker '{marker}' not found starting from {current}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = find_project_root()\n",
    "DATA_PATH = root / \"dataset\"\n",
    "SAVED_DATA_PATH = root / \"dataset/dsv4.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the old preprocess_V2 data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sep_col = [\n",
    "    \"BaseExcess\",\n",
    "    \"HCO3\",\n",
    "    \"FiO2\",\n",
    "    \"pH\",\n",
    "    \"PaCO2\",\n",
    "    \"SaO2\",\n",
    "    \"AST\",\n",
    "    \"BUN\",\n",
    "    \"Alkalinephos\",\n",
    "    \"Calcium\",\n",
    "    \"Chloride\",\n",
    "    \"Creatinine\",\n",
    "    \"Glucose\",\n",
    "    \"Lactate\",\n",
    "    \"Magnesium\",\n",
    "    \"Phosphate\",\n",
    "    \"Potassium\",\n",
    "    \"Bilirubin_total\",\n",
    "    \"Hct\",\n",
    "    \"Hgb\",\n",
    "    \"PTT\",\n",
    "    \"WBC\",\n",
    "    \"Platelets\",\n",
    "    \"Bilirubin_direct\",\n",
    "    \"Fibrinogen\",\n",
    "]\n",
    "\n",
    "# Continues Health Indicators\n",
    "con_col = [\"HR\", \"O2Sat\", \"Temp\", \"SBP\", \"MAP\", \"DBP\", \"Resp\", \"EtCO2\"]\n",
    "\n",
    "\n",
    "def feature_missing_information(patient_data, columns):\n",
    "    # temp_data holds the information from the patient file as well as the features that will be calculated\n",
    "    temp_data = np.array(patient_data)\n",
    "\n",
    "    # Calculate 3 features for each column, 2 respective of the frequency of NaN values and 1 respective of the change in recorded values\n",
    "    for column in columns:\n",
    "        data = np.array(patient_data[column])\n",
    "        nan_pos = np.where(~np.isnan(data))[0]\n",
    "\n",
    "        # Measurement frequency sequence\n",
    "        interval_f1 = data.copy()\n",
    "        # Measurement time interval\n",
    "        interval_f2 = data.copy()\n",
    "\n",
    "        # If all the values are NaN\n",
    "        if len(nan_pos) == 0:\n",
    "            interval_f1[:] = 0\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "            interval_f2[:] = -1\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "        else:\n",
    "            # Puts number of measurements into temp_data\n",
    "            interval_f1[: nan_pos[0]] = 0\n",
    "            for p in range(len(nan_pos) - 1):\n",
    "                interval_f1[nan_pos[p] : nan_pos[p + 1]] = p + 1\n",
    "            interval_f1[nan_pos[-1] :] = len(nan_pos)\n",
    "            temp_data = np.column_stack((temp_data, interval_f1))\n",
    "\n",
    "            # Puts the frequency of measurements into temp_data\n",
    "            interval_f2[: nan_pos[0]] = -1\n",
    "            for q in range(len(nan_pos) - 1):\n",
    "                length = nan_pos[q + 1] - nan_pos[q]\n",
    "                for l in range(length):\n",
    "                    interval_f2[nan_pos[q] + l] = l\n",
    "\n",
    "            length = len(patient_data) - nan_pos[-1]\n",
    "            for l in range(length):\n",
    "                interval_f2[nan_pos[-1] + l] = l\n",
    "            temp_data = np.column_stack((temp_data, interval_f2))\n",
    "\n",
    "        # Differential features\n",
    "        # These capture the change in values that have been recorded (quite simply as well but it should be just fine)\n",
    "        diff_f = data.copy()\n",
    "        diff_f = diff_f.astype(float)\n",
    "        if len(nan_pos) <= 1:\n",
    "            diff_f[:] = np.nan\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "        else:\n",
    "            diff_f[: nan_pos[1]] = np.nan\n",
    "            for p in range(1, len(nan_pos) - 1):\n",
    "                diff_f[nan_pos[p] : nan_pos[p + 1]] = (\n",
    "                    data[nan_pos[p]] - data[nan_pos[p - 1]]\n",
    "                )\n",
    "            diff_f[nan_pos[-1] :] = data[nan_pos[-1]] - data[nan_pos[-2]]\n",
    "            temp_data = np.column_stack((temp_data, diff_f))\n",
    "\n",
    "    return temp_data\n",
    "\n",
    "\n",
    "def feature_slide_window(patient_data, columns):\n",
    "\n",
    "    window_size = 6\n",
    "    features = {}\n",
    "\n",
    "    for column in columns:\n",
    "        series = patient_data[column]\n",
    "\n",
    "        features[f\"{column}_max\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).max()\n",
    "        features[f\"{column}_min\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).min()\n",
    "        features[f\"{column}_mean\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).mean()\n",
    "        features[f\"{column}_median\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).median()\n",
    "        features[f\"{column}_std\"] = series.rolling(\n",
    "            window=window_size, min_periods=1\n",
    "        ).std()\n",
    "\n",
    "        # For calculating std dev of differences, use diff() then apply rolling std\n",
    "        diff_std = series.diff().rolling(window=window_size, min_periods=1).std()\n",
    "        features[f\"{column}_diff_std\"] = diff_std\n",
    "\n",
    "    # Convert the dictionary of features into a DataFrame\n",
    "    features_df = pd.DataFrame(features)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "def features_score(patient_data):\n",
    "    \"\"\"\n",
    "    Gives score assocciated with the patient data according to the scoring systems of NEWS, SOFA and qSOFA\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.zeros((len(patient_data), 8))\n",
    "\n",
    "    for ii in range(len(patient_data)):\n",
    "        HR = patient_data[ii, 0]\n",
    "        if HR == np.nan:\n",
    "            HR_score = np.nan\n",
    "        elif (HR <= 40) | (HR >= 131):\n",
    "            HR_score = 3\n",
    "        elif 111 <= HR <= 130:\n",
    "            HR_score = 2\n",
    "        elif (41 <= HR <= 50) | (91 <= HR <= 110):\n",
    "            HR_score = 1\n",
    "        else:\n",
    "            HR_score = 0\n",
    "        scores[ii, 0] = HR_score\n",
    "\n",
    "        Temp = patient_data[ii, 2]\n",
    "        if Temp == np.nan:\n",
    "            Temp_score = np.nan\n",
    "        elif Temp <= 35:\n",
    "            Temp_score = 3\n",
    "        elif Temp >= 39.1:\n",
    "            Temp_score = 2\n",
    "        elif (35.1 <= Temp <= 36.0) | (38.1 <= Temp <= 39.0):\n",
    "            Temp_score = 1\n",
    "        else:\n",
    "            Temp_score = 0\n",
    "        scores[ii, 1] = Temp_score\n",
    "\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if Resp == np.nan:\n",
    "            Resp_score = np.nan\n",
    "        elif (Resp < 8) | (Resp > 25):\n",
    "            Resp_score = 3\n",
    "        elif 21 <= Resp <= 24:\n",
    "            Resp_score = 2\n",
    "        elif 9 <= Resp <= 11:\n",
    "            Resp_score = 1\n",
    "        else:\n",
    "            Resp_score = 0\n",
    "        scores[ii, 2] = Resp_score\n",
    "\n",
    "        Creatinine = patient_data[ii, 19]\n",
    "        if Creatinine == np.nan:\n",
    "            Creatinine_score = np.nan\n",
    "        elif Creatinine < 1.2:\n",
    "            Creatinine_score = 0\n",
    "        elif Creatinine < 2:\n",
    "            Creatinine_score = 1\n",
    "        elif Creatinine < 3.5:\n",
    "            Creatinine_score = 2\n",
    "        else:\n",
    "            Creatinine_score = 3\n",
    "        scores[ii, 3] = Creatinine_score\n",
    "\n",
    "        MAP = patient_data[ii, 4]\n",
    "        if MAP == np.nan:\n",
    "            MAP_score = np.nan\n",
    "        elif MAP >= 70:\n",
    "            MAP_score = 0\n",
    "        else:\n",
    "            MAP_score = 1\n",
    "        scores[ii, 4] = MAP_score\n",
    "\n",
    "        SBP = patient_data[ii, 3]\n",
    "        Resp = patient_data[ii, 6]\n",
    "        if SBP + Resp == np.nan:\n",
    "            qsofa = np.nan\n",
    "        elif (SBP <= 100) & (Resp >= 22):\n",
    "            qsofa = 1\n",
    "        else:\n",
    "            qsofa = 0\n",
    "        scores[ii, 5] = qsofa\n",
    "\n",
    "        Platelets = patient_data[ii, 30]\n",
    "        if Platelets == np.nan:\n",
    "            Platelets_score = np.nan\n",
    "        elif Platelets <= 50:\n",
    "            Platelets_score = 3\n",
    "        elif Platelets <= 100:\n",
    "            Platelets_score = 2\n",
    "        elif Platelets <= 150:\n",
    "            Platelets_score = 1\n",
    "        else:\n",
    "            Platelets_score = 0\n",
    "        scores[ii, 6] = Platelets_score\n",
    "\n",
    "        Bilirubin = patient_data[ii, 25]\n",
    "        if Bilirubin == np.nan:\n",
    "            Bilirubin_score = np.nan\n",
    "        elif Bilirubin < 1.2:\n",
    "            Bilirubin_score = 0\n",
    "        elif Bilirubin < 2:\n",
    "            Bilirubin_score = 1\n",
    "        elif Bilirubin < 6:\n",
    "            Bilirubin_score = 2\n",
    "        else:\n",
    "            Bilirubin_score = 3\n",
    "        scores[ii, 7] = Bilirubin_score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def extract_features(patient_data, columns_to_drop=[]):\n",
    "    # Get the column with Sepsis Label as it is not the same for each row (check documentation)\n",
    "    labels = np.array(patient_data[\"SepsisLabel\"])\n",
    "    patient_data = patient_data.drop(columns=columns_to_drop)\n",
    "\n",
    "    # Gets information from the missing variables\n",
    "    # This can be useful as it shows the clinical judgment, the test has not been ordered\n",
    "    #                              (probably a good decision we should take into account)\n",
    "    temp_data = feature_missing_information(patient_data, sep_col + con_col)\n",
    "    temp = pd.DataFrame(temp_data)\n",
    "    # To complete the data use forward-filling strategy\n",
    "    temp = temp.fillna(method=\"ffill\")\n",
    "    # These are also the first set of features\n",
    "    # In this configutation 99 (66 + 33 or 3 per column) features to be precise\n",
    "    # They are also time indifferent\n",
    "    features_A = np.array(temp)\n",
    "    # The team did not use DBP, not sure why, might investigate this\n",
    "    # columns = ['HR', 'O2Sat', 'SBP', 'MAP', 'Resp', 'DBP']\n",
    "\n",
    "    # six-hour slide window statistics of selected columns\n",
    "    columns = [\"HR\", \"O2Sat\", \"SBP\", \"MAP\", \"Resp\"]\n",
    "    features_B = feature_slide_window(patient_data, columns)\n",
    "\n",
    "    # Score features based according to NEWS, SOFA and qSOFA\n",
    "    features_C = features_score(features_A)\n",
    "\n",
    "    features = np.column_stack([features_A, features_B, features_C])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "# Data Pre-processing\n",
    "def preprecess_data(dataset, patient_id_map=None):\n",
    "    frames_features = []\n",
    "    frames_labels = []\n",
    "\n",
    "    for patient_id in set(dataset.index.get_level_values(0)):\n",
    "        if patient_id_map is not None:\n",
    "            print(\n",
    "                f\"Processing data for patient ID: {patient_id}, File: {patient_id_map[patient_id]}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        patient_data = dataset.loc[patient_id]\n",
    "\n",
    "        features, labels = extract_features(patient_data)\n",
    "        features = pd.DataFrame(features)\n",
    "        labels = pd.DataFrame(labels)\n",
    "\n",
    "        frames_features.append(features)\n",
    "        frames_labels.append(labels)\n",
    "\n",
    "    data_features = np.array(pd.concat(frames_features))\n",
    "    data_labels = (np.array(pd.concat(frames_labels)))[:, 0]\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    index = [i for i in range(len(data_labels))]\n",
    "    np.random.shuffle(index)\n",
    "    data_features = data_features[index]\n",
    "    data_labels = data_labels[index]\n",
    "\n",
    "    return data_features, data_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset():\n",
    "    data = []\n",
    "    patient_ids = np.array([])\n",
    "    file_listA = os.listdir(DATA_PATH / 'training_setA')\n",
    "    file_listB = os.listdir(DATA_PATH / 'training_setB')\n",
    "    patient_counter = 1  # Start counter for patient IDs\n",
    "    patient_id_map = {}  # Map to store counter to filename mapping\n",
    "    \n",
    "    # Process files in training_setA\n",
    "    for file_name in file_listA:\n",
    "        file_path = os.path.join(DATA_PATH, 'training_setA', file_name)\n",
    "        df_temp = pd.read_csv(file_path, sep='|')\n",
    "        patient_ids = np.append(patient_ids, [patient_counter] * len(df_temp))  # Use counter as patient ID\n",
    "        # patient_ids.extends([patient_counter] * len(df_temp))  # Use counter as patient ID\n",
    "        data.append(df_temp)\n",
    "        patient_id_map[patient_counter] = file_name  # Map counter to filename\n",
    "        patient_counter += 1  # Increment counter for the next patient\n",
    "        print(\"  \", patient_counter, end='\\r')\n",
    "    print(\"  \", patient_counter)\n",
    "\n",
    "    # Process files in training_setB\n",
    "    for file_name in file_listB:\n",
    "        file_path = os.path.join(DATA_PATH, 'training_setB', file_name)\n",
    "        df_temp = pd.read_csv(file_path, sep='|')\n",
    "        patient_ids = np.append(patient_ids, [patient_counter] * len(df_temp))\n",
    "        # patient_ids.extend([patient_counter] * len(df_temp))  # Use counter as patient ID\n",
    "        data.append(df_temp)\n",
    "        patient_id_map[patient_counter] = file_name  # Map counter to filename\n",
    "        patient_counter += 1  # Increment counter for the next patient\n",
    "        print(\"  \", patient_counter, end='\\r')\n",
    "    print(\"  \", patient_counter)\n",
    "\n",
    "    combined_df = pd.concat(data, ignore_index=True)\n",
    "    combined_df['patient_id'] = patient_ids\n",
    "    combined_df.set_index(['patient_id', combined_df.index], inplace=True)\n",
    "    \n",
    "    print(\"Dataset loaded into a MultiIndex DataFrame.\")\n",
    "    return combined_df, patient_id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_linear_interpolation(data, column_name):\n",
    "    \"\"\"\n",
    "    Imputes missing values using linear interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The data frame containing the column values.\n",
    "    - column_name (str): The name of the column to impute.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with missing values imputed using linear interpolation.\n",
    "    \"\"\"\n",
    "    imputed_data = data\n",
    "    imputed_data[column_name] = data[column_name].interpolate(method='linear')\n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20337\n",
      "   40337\n",
      "Dataset loaded into a MultiIndex DataFrame.\n",
      "Dataset initially:  (1552210, 41)\n",
      "Linearly interpolating:\n",
      "HR\n",
      "O2Sat\n",
      "SBP\n",
      "MAP\n",
      "DBP\n",
      "Resp\n",
      "Done\n",
      "Seeing if there are still any nan values or +/- infinities\n",
      "Data contains NaN or infinite values. Handling...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "dataset, patient_id_map = get_dataset()\n",
    "\n",
    "print(\"Dataset initially: \", dataset.shape)\n",
    "\n",
    "downsampling = False\n",
    "if downsampling:\n",
    "    sepsis_groups = dataset.groupby(level=\"patient_id\")[\"SepsisLabel\"].max()\n",
    "    patients_sepsis = sepsis_groups[sepsis_groups == 1].index\n",
    "    patients_no_sepsis = sepsis_groups[sepsis_groups == 0].index\n",
    "    min_size = len(patients_sepsis)\n",
    "    sampled_no_sepsis = np.random.choice(\n",
    "        patients_no_sepsis, min_size, replace=False\n",
    "    )\n",
    "    dataset = dataset.loc[np.concatenate([patients_sepsis, sampled_no_sepsis])]\n",
    "    print(\"Dataset after downsampling: \", dataset.shape)\n",
    "\n",
    "\n",
    "columns_to_linearly_interpolate = [\"HR\", \"O2Sat\", \"SBP\", \"MAP\", \"DBP\", \"Resp\"]\n",
    "\n",
    "\n",
    "# Linear Interpolation\n",
    "print(\"Linearly interpolating:\")\n",
    "for col in columns_to_linearly_interpolate:\n",
    "    if col != \"SepsisLabel\":  # Ensure we do not interpolate 'SepsisLabel'\n",
    "        dataset = impute_linear_interpolation(dataset, col)\n",
    "        print(col)\n",
    "print(\"Done\")\n",
    "\n",
    "# ___________________________________________________________________________________________________________________________________\n",
    "\n",
    "# ___________________________________________________________________________________________________________________________________\n",
    "\n",
    "def add_nan_indicators(df):\n",
    "    for column in df.columns:\n",
    "        df[column + \"_nan\"] = df[column].isna().astype(int)\n",
    "    return df\n",
    "\n",
    "def downsample(X, y):\n",
    "    index_0 = np.where(y == 0)[0]\n",
    "    index_1 = np.where(y == 1)[0]\n",
    "    print(index_0, index_1)\n",
    "\n",
    "    if len(index_0) > len(index_1):\n",
    "        index_0 = np.random.choice(index_0, size=len(index_1), replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([index_0, index_1])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    x_balanced = X.iloc[balanced_indices]\n",
    "    y_balanced = y.iloc[balanced_indices]\n",
    "\n",
    "    return x_balanced, y_balanced\n",
    "\n",
    "# ___________________________________________________________________________________________________________________________________\n",
    "\n",
    "feature_engineer = True\n",
    "if feature_engineer:\n",
    "    X = add_nan_indicators(dataset)\n",
    "    XX, y = preprecess_data(X)\n",
    "    new_feature_names = [f\"new_feature_{i}\" for i in range(XX.shape[1])]\n",
    "    XX_df = pd.DataFrame(XX, columns=new_feature_names, index=X.index)\n",
    "\n",
    "    # Concatenate the new features from XX_df back to the original DataFrame X\n",
    "    X = pd.concat([X, XX_df], axis=1)\n",
    "    y = dataset[\"SepsisLabel\"]\n",
    "\n",
    "else:\n",
    "    X = dataset.drop(\"SepsisLabel\", axis=1)\n",
    "    X = add_nan_indicators(X)\n",
    "    y = dataset[\"SepsisLabel\"]\n",
    "\n",
    "# just in case\n",
    "dataset *= 0\n",
    "\n",
    "print(\"Seeing if there are still any nan values or +/- infinities\")\n",
    "# Just trying to fix some errors I got only on a GPU\n",
    "# if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "#     print(\"Data contains NaN or infinite values. Handling...\")\n",
    "#     X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "#     X.fillna(method='ffill', inplace=True)\n",
    "if X.isin([np.nan, np.inf, -np.inf]).any().any():\n",
    "    print(\"Data contains NaN or infinite values. Handling...\")\n",
    "    # Replace infinite values with NaN so they can be filled too\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # First apply forward fill\n",
    "    X.fillna(method=\"ffill\", inplace=True)\n",
    "    # Then apply backward fill for any remaining NaNs\n",
    "    X.fillna(method=\"bfill\", inplace=True)\n",
    "\n",
    "# Ensure no NaNs or infinities in the target variable as well\n",
    "if y.isin([np.nan, np.inf, -np.inf]).any():\n",
    "    print(\"Target contains NaN or infinite values. Handling...\")\n",
    "    y.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    y.fillna(method=\"ffill\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with labels saved to /Users/aidend/Developer/ds_uni/dl-sepsis-prediction/dataset/dsv4.parquet\n"
     ]
    }
   ],
   "source": [
    "df_save = X.copy()\n",
    "df_save[\"SepsisLabel\"] = y\n",
    "#reset index   \n",
    "df_save.reset_index(drop=False, inplace=True)\n",
    "df_save.to_parquet(SAVED_DATA_PATH)\n",
    "print(f\"Dataset with labels saved to {SAVED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552210, 303)\n"
     ]
    }
   ],
   "source": [
    "print(df_save.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>level_1</th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>...</th>\n",
       "      <th>new_feature_209</th>\n",
       "      <th>new_feature_210</th>\n",
       "      <th>new_feature_211</th>\n",
       "      <th>new_feature_212</th>\n",
       "      <th>new_feature_213</th>\n",
       "      <th>new_feature_214</th>\n",
       "      <th>new_feature_215</th>\n",
       "      <th>new_feature_216</th>\n",
       "      <th>new_feature_217</th>\n",
       "      <th>new_feature_218</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.50</td>\n",
       "      <td>121.00</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.00</td>\n",
       "      <td>13.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.298503</td>\n",
       "      <td>1.486046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.25</td>\n",
       "      <td>113.25</td>\n",
       "      <td>61.0</td>\n",
       "      <td>41.50</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.029026</td>\n",
       "      <td>4.732864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.25</td>\n",
       "      <td>132.75</td>\n",
       "      <td>71.5</td>\n",
       "      <td>46.25</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.430258</td>\n",
       "      <td>5.609516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>78.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>103.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>43.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974842</td>\n",
       "      <td>2.875181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>128.75</td>\n",
       "      <td>69.5</td>\n",
       "      <td>44.50</td>\n",
       "      <td>12.5</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.167948</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  level_1    HR  O2Sat   Temp     SBP   MAP    DBP  Resp  EtCO2  \\\n",
       "0         1.0        0  80.0  100.0  36.50  121.00  58.0  41.00  13.5   34.0   \n",
       "1         1.0        1  76.0  100.0  36.25  113.25  61.0  41.50  12.0   34.0   \n",
       "2         1.0        2  80.0  100.0  36.25  132.75  71.5  46.25  12.0   34.0   \n",
       "3         1.0        3  78.0  100.0  36.10  103.50  58.0  43.00  12.0   34.0   \n",
       "4         1.0        4  74.0  100.0  36.00  128.75  69.5  44.50  12.5   34.0   \n",
       "\n",
       "   ...  new_feature_209  new_feature_210  new_feature_211  new_feature_212  \\\n",
       "0  ...         1.298503         1.486046              0.0              1.0   \n",
       "1  ...         3.029026         4.732864              0.0              0.0   \n",
       "2  ...         3.430258         5.609516              0.0              0.0   \n",
       "3  ...         1.974842         2.875181              0.0              1.0   \n",
       "4  ...         2.167948         1.264911              1.0              0.0   \n",
       "\n",
       "   new_feature_213  new_feature_214  new_feature_215  new_feature_216  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              1.0              0.0              0.0   \n",
       "2              2.0              0.0              0.0              0.0   \n",
       "3              0.0              3.0              0.0              0.0   \n",
       "4              0.0              0.0              1.0              0.0   \n",
       "\n",
       "   new_feature_217  new_feature_218  \n",
       "0              3.0              2.0  \n",
       "1              3.0              2.0  \n",
       "2              3.0              2.0  \n",
       "3              0.0              3.0  \n",
       "4              3.0              2.0  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('float64') dtype('int64')]\n"
     ]
    }
   ],
   "source": [
    "# print unique data types\n",
    "print(df_save.dtypes.unique())\n",
    "# print unique values\n",
    "# print(df_save.nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the columns to a json\n",
    "columns_json = df_save.columns.tolist()\n",
    "with open('columns.json', 'w') as f:\n",
    "    json.dump(columns_json, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          1\n",
       "1          1\n",
       "2          1\n",
       "3          1\n",
       "4          1\n",
       "          ..\n",
       "1552205    0\n",
       "1552206    0\n",
       "1552207    0\n",
       "1552208    0\n",
       "1552209    0\n",
       "Name: Gender, Length: 1552210, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save[\"Gender\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.000e+00,  1.000e+00,  0.000e+00, ..., -9.090e+02, -4.120e+02,\n",
       "       -1.784e+03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_save[\"new_feature_102\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
