{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates the final datasets from the preprocessed dataset. It splits the data into train, val and test sets and applies various sampling methods for the train set: no_sampling, upsampling and downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_NAME = \"V2_preprocessed\"\n",
    "OUTPUT_FOLDER_NAME = \"final_datasets\"\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "VAL_SIZE = 0.125\n",
    "SAMPLING_MINORITY_RATIO = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.utils import resample\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(marker=\".gitignore\"):\n",
    "    \"\"\"\n",
    "    walk up from the current working directory until a directory containing the\n",
    "    specified marker (e.g., .gitignore) is found.\n",
    "    \"\"\"\n",
    "    current = Path.cwd()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / marker).exists():\n",
    "            return parent.resolve()\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project root marker '{marker}' not found starting from {current}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/aidend/Developer/ds_uni/dl-sepsis-prediction\n"
     ]
    }
   ],
   "source": [
    "project_root = find_project_root()\n",
    "print(\"Project root:\", project_root)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "df_path = f\"{project_root}/dataset/{INPUT_FILE_NAME}.parquet\"\n",
    "try:\n",
    "    df = pd.read_parquet(df_path)\n",
    "except Exception as e:\n",
    "    sys.exit(f\"Error loading dataset from {df_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_under_sample(df, method=\"oversample\", minority_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Balances the dataset at the patient level.\n",
    "\n",
    "    Each patient's overall sepsis label is taken as the maximum value\n",
    "    (if any record shows sepsis, the patient is marked as septic).\n",
    "\n",
    "    We then either oversample the septic (minority) patients or undersample\n",
    "    the non-septic (majority) patients to change the ratio.\n",
    "\n",
    "    In the final dataset, each copy of a patient gets a unique ID so that\n",
    "    oversampled patients appear as separate instances.\n",
    "    \"\"\"\n",
    "    # Create a patient-level summary with one record per patient.\n",
    "    patient_df = df.groupby(\"patient_id\")[\"SepsisLabel\"].max().reset_index()\n",
    "\n",
    "    # Count patients in each group.\n",
    "    counts = patient_df[\"SepsisLabel\"].value_counts()\n",
    "    majority_class = counts.idxmax()\n",
    "    minority_class = counts.idxmin()\n",
    "\n",
    "    # Split the patients into majority and minority groups.\n",
    "    majority_patients = patient_df[patient_df[\"SepsisLabel\"] == majority_class]\n",
    "    minority_patients = patient_df[patient_df[\"SepsisLabel\"] == minority_class]\n",
    "\n",
    "    # Resample based on the chosen method.\n",
    "    if method == \"oversample\":\n",
    "        # Duplicate minority patients to reach desired ratio.\n",
    "        n_desired_minority = int(\n",
    "            (minority_ratio * len(majority_patients)) / (1 - minority_ratio)\n",
    "        )\n",
    "        minority_upsampled = resample(\n",
    "            minority_patients,\n",
    "            replace=True,\n",
    "            n_samples=n_desired_minority,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        balanced_patient_df = pd.concat([majority_patients, minority_upsampled])\n",
    "    elif method == \"undersample\":\n",
    "        # Remove some majority patients to reach desired ratio.\n",
    "        n_desired_majority = int(\n",
    "            ((1 - minority_ratio) / minority_ratio) * len(minority_patients)\n",
    "        )\n",
    "        majority_downsampled = resample(\n",
    "            majority_patients,\n",
    "            replace=False,\n",
    "            n_samples=n_desired_majority,\n",
    "            random_state=RANDOM_STATE,\n",
    "        )\n",
    "        balanced_patient_df = pd.concat([majority_downsampled, minority_patients])\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'oversample' or 'undersample'\")\n",
    "\n",
    "    # Rebuild the full dataset with patient records.\n",
    "    # If a patient appears more than once due to resampling,\n",
    "    # assign a new unique patient ID to each duplicate.\n",
    "    final_dfs = []\n",
    "    patient_occurrences = {}\n",
    "\n",
    "    for pid in balanced_patient_df[\"patient_id\"]:\n",
    "        # Get all records for this patient.\n",
    "        patient_records = df[df[\"patient_id\"] == pid].copy()\n",
    "        # Count how many times this patient has been added.\n",
    "        if pid in patient_occurrences:\n",
    "            patient_occurrences[pid] += 1\n",
    "            # Create a new unique ID by appending a suffix.\n",
    "            new_pid = pid * 1000 + patient_occurrences[pid]\n",
    "            patient_records[\"patient_id\"] = new_pid\n",
    "        else:\n",
    "            # first occurrence, keep original ID\n",
    "            patient_occurrences[pid] = 0\n",
    "        final_dfs.append(patient_records)\n",
    "\n",
    "    balanced_df = pd.concat(final_dfs, ignore_index=True)\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train, val, and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_labels = df.groupby(\"patient_id\")[\"SepsisLabel\"].max()\n",
    "\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    patient_labels.index,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=patient_labels,  # Stratify by patient-level labels\n",
    ")\n",
    "\n",
    "train_val_df = df[df[\"patient_id\"].isin(train_val_patients)]\n",
    "test_df = df[df[\"patient_id\"].isin(test_patients)]\n",
    "\n",
    "train_patients, val_patients = train_test_split(\n",
    "        train_val_patients,\n",
    "        test_size=VAL_SIZE,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=patient_labels[train_val_patients],\n",
    ")\n",
    "\n",
    "train_df = train_val_df[train_val_df[\"patient_id\"].isin(train_patients)]\n",
    "val_df = train_val_df[train_val_df[\"patient_id\"].isin(val_patients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     3     4 ... 40333 40334 40335]\n",
      "1 40335\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"patient_id\"].unique())\n",
    "# print min and max of the patient ids\n",
    "print(np.min(train_df[\"patient_id\"]), np.max(train_df[\"patient_id\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "def scale_dfs(train_df, val_df, test_df, labels=[\"SepsisLabel\", \"patient_id\"], eps=1e-6):\n",
    "    # Identify feature columns\n",
    "    feature_cols = [c for c in train_df.columns if c not in labels]\n",
    "\n",
    "    # Fit scaler on train features\n",
    "    scaler = StandardScaler().fit(train_df[feature_cols])\n",
    "\n",
    "    # Apply scaling\n",
    "    def apply_scaling(df):\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[feature_cols] = scaler.transform(df[feature_cols])\n",
    "        return df_scaled\n",
    "\n",
    "    train_scaled = apply_scaling(train_df)\n",
    "    val_scaled   = apply_scaling(val_df)\n",
    "    test_scaled  = apply_scaling(test_df)\n",
    "    \n",
    "    patient_id_index = train_df.columns.get_loc(\"patient_id\")\n",
    "    with open(f\"{project_root}/dataset/{OUTPUT_FOLDER_NAME}/patient_id_index.pkl\", \"wb\") as f:\n",
    "        pickle.dump(patient_id_index, f)\n",
    "\n",
    "    df_shape = train_df[feature_cols].shape\n",
    "    with open(f\"{project_root}/dataset/{OUTPUT_FOLDER_NAME}/df_shape.pkl\", \"wb\") as f:\n",
    "        pickle.dump(df_shape, f)\n",
    "\n",
    "\n",
    "    # --- TEST: mean ≈ 0 and std ≈ 1 on train_scaled ---\n",
    "    means = train_scaled[feature_cols].mean()\n",
    "    stds  = train_scaled[feature_cols].std(ddof=0)  # population std to match StandardScaler\n",
    "\n",
    "    # Check each column\n",
    "    mean_pass = (means.abs() < eps).all()\n",
    "    std_pass  = ((stds - 1).abs() < eps).all()\n",
    "\n",
    "    # Print detailed results\n",
    "    print(\"Scaling test results on train set:\")\n",
    "    df_test = pd.DataFrame({\n",
    "        'mean': means.round(6),\n",
    "        'std':  stds.round(6),\n",
    "        'mean_ok': (means.abs() < eps),\n",
    "        'std_ok':  ((stds - 1).abs() < eps)\n",
    "    })\n",
    "    print(df_test)\n",
    "    print(f\"\\nAll means ≈ 0? {'✅' if mean_pass else '❌'}\")\n",
    "    print(f\"All stds ≈ 1?  {'✅' if std_pass else '❌'}\\n\")\n",
    "    \n",
    "    # Save the scaler\n",
    "    scaler_path = f\"{project_root}/dataset/{OUTPUT_FOLDER_NAME}/scaler.pkl\"\n",
    "    with open(scaler_path, \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    return train_scaled, val_scaled, test_scaled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling test results on train set:\n",
      "                  mean  std  mean_ok  std_ok\n",
      "HR                 0.0  1.0     True    True\n",
      "O2Sat             -0.0  1.0     True    True\n",
      "Temp              -0.0  1.0     True    True\n",
      "SBP                0.0  1.0     True    True\n",
      "MAP                0.0  1.0     True    True\n",
      "...                ...  ...      ...     ...\n",
      "Resp_min_6h        0.0  1.0     True    True\n",
      "Resp_mean_6h      -0.0  1.0     True    True\n",
      "Resp_median_6h     0.0  1.0     True    True\n",
      "Resp_std_6h        0.0  1.0     True    True\n",
      "Resp_diff_std_6h  -0.0  1.0     True    True\n",
      "\n",
      "[107 rows x 4 columns]\n",
      "\n",
      "All means ≈ 0? ✅\n",
      "All stds ≈ 1?  ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = scale_dfs(train_df, val_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: patient_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "patient_id_index = train_df.columns.get_loc(\"patient_id\")\n",
    "#patient ids\n",
    "patient_ids = train_df.iloc[:, patient_id_index]\n",
    "print(patient_ids.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28234\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"patient_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Temp</th>\n",
       "      <th>SBP</th>\n",
       "      <th>MAP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>Resp</th>\n",
       "      <th>EtCO2</th>\n",
       "      <th>BaseExcess</th>\n",
       "      <th>HCO3</th>\n",
       "      <th>...</th>\n",
       "      <th>MAP_mean_6h</th>\n",
       "      <th>MAP_median_6h</th>\n",
       "      <th>MAP_std_6h</th>\n",
       "      <th>MAP_diff_std_6h</th>\n",
       "      <th>Resp_max_6h</th>\n",
       "      <th>Resp_min_6h</th>\n",
       "      <th>Resp_mean_6h</th>\n",
       "      <th>Resp_median_6h</th>\n",
       "      <th>Resp_std_6h</th>\n",
       "      <th>Resp_diff_std_6h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.022785</td>\n",
       "      <td>-1.887699</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>0.198027</td>\n",
       "      <td>0.343221</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>1.192701</td>\n",
       "      <td>-0.536513</td>\n",
       "      <td>0.271187</td>\n",
       "      <td>-0.978945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389594</td>\n",
       "      <td>0.408017</td>\n",
       "      <td>0.404621</td>\n",
       "      <td>0.981094</td>\n",
       "      <td>0.534639</td>\n",
       "      <td>2.139701</td>\n",
       "      <td>1.426581</td>\n",
       "      <td>1.415296</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>1.192096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727362</td>\n",
       "      <td>-0.708370</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-1.120443</td>\n",
       "      <td>-0.453785</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>0.063230</td>\n",
       "      <td>-0.536513</td>\n",
       "      <td>0.271187</td>\n",
       "      <td>-0.978945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067477</td>\n",
       "      <td>-0.044764</td>\n",
       "      <td>0.404621</td>\n",
       "      <td>0.981094</td>\n",
       "      <td>0.534639</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.758024</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.940694</td>\n",
       "      <td>1.192096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.264725</td>\n",
       "      <td>0.609247</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-0.071440</td>\n",
       "      <td>0.206996</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>0.656412</td>\n",
       "      <td>-0.536513</td>\n",
       "      <td>0.271187</td>\n",
       "      <td>-0.978945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032798</td>\n",
       "      <td>0.253237</td>\n",
       "      <td>-0.031841</td>\n",
       "      <td>0.981094</td>\n",
       "      <td>0.534639</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>0.769249</td>\n",
       "      <td>0.797620</td>\n",
       "      <td>0.267715</td>\n",
       "      <td>1.192096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322555</td>\n",
       "      <td>-0.708370</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-0.071440</td>\n",
       "      <td>0.372036</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>2.238232</td>\n",
       "      <td>-0.536513</td>\n",
       "      <td>14.432587</td>\n",
       "      <td>-0.978945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130259</td>\n",
       "      <td>0.330627</td>\n",
       "      <td>-0.155709</td>\n",
       "      <td>0.349022</td>\n",
       "      <td>1.489872</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>1.243017</td>\n",
       "      <td>1.106458</td>\n",
       "      <td>1.301121</td>\n",
       "      <td>1.505821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.074340</td>\n",
       "      <td>-2.849497</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-0.071440</td>\n",
       "      <td>0.537076</td>\n",
       "      <td>0.296926</td>\n",
       "      <td>1.150731</td>\n",
       "      <td>-0.536513</td>\n",
       "      <td>0.271187</td>\n",
       "      <td>-0.978945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226595</td>\n",
       "      <td>0.408017</td>\n",
       "      <td>-0.164923</td>\n",
       "      <td>0.062996</td>\n",
       "      <td>1.489872</td>\n",
       "      <td>0.765886</td>\n",
       "      <td>1.269793</td>\n",
       "      <td>1.366956</td>\n",
       "      <td>0.949284</td>\n",
       "      <td>1.421386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HR     O2Sat      Temp       SBP       MAP       DBP      Resp  \\\n",
       "0  1.022785 -1.887699 -0.069259  0.198027  0.343221  0.296926  1.192701   \n",
       "1  0.727362 -0.708370 -0.069259 -1.120443 -0.453785  0.296926  0.063230   \n",
       "2  0.264725  0.609247 -0.069259 -0.071440  0.206996  0.296926  0.656412   \n",
       "3  0.322555 -0.708370 -0.069259 -0.071440  0.372036  0.296926  2.238232   \n",
       "4  1.074340 -2.849497 -0.069259 -0.071440  0.537076  0.296926  1.150731   \n",
       "\n",
       "      EtCO2  BaseExcess      HCO3  ...  MAP_mean_6h  MAP_median_6h  \\\n",
       "0 -0.536513    0.271187 -0.978945  ...     0.389594       0.408017   \n",
       "1 -0.536513    0.271187 -0.978945  ...    -0.067477      -0.044764   \n",
       "2 -0.536513    0.271187 -0.978945  ...     0.032798       0.253237   \n",
       "3 -0.536513   14.432587 -0.978945  ...     0.130259       0.330627   \n",
       "4 -0.536513    0.271187 -0.978945  ...     0.226595       0.408017   \n",
       "\n",
       "   MAP_std_6h  MAP_diff_std_6h  Resp_max_6h  Resp_min_6h  Resp_mean_6h  \\\n",
       "0    0.404621         0.981094     0.534639     2.139701      1.426581   \n",
       "1    0.404621         0.981094     0.534639     0.765886      0.758024   \n",
       "2   -0.031841         0.981094     0.534639     0.765886      0.769249   \n",
       "3   -0.155709         0.349022     1.489872     0.765886      1.243017   \n",
       "4   -0.164923         0.062996     1.489872     0.765886      1.269793   \n",
       "\n",
       "   Resp_median_6h  Resp_std_6h  Resp_diff_std_6h  \n",
       "0        1.415296     0.940694          1.192096  \n",
       "1        0.764857     0.940694          1.192096  \n",
       "2        0.797620     0.267715          1.192096  \n",
       "3        1.106458     1.301121          1.505821  \n",
       "4        1.366956     0.949284          1.421386  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR                  1.349512e-16\n",
      "O2Sat              -1.549835e-14\n",
      "Temp               -2.984082e-14\n",
      "SBP                 1.213409e-15\n",
      "MAP                 9.135644e-16\n",
      "                        ...     \n",
      "Resp_min_6h         1.314427e-15\n",
      "Resp_mean_6h       -1.923395e-15\n",
      "Resp_median_6h      2.073266e-15\n",
      "Resp_std_6h         3.739688e-16\n",
      "Resp_diff_std_6h   -4.728005e-16\n",
      "Length: 109, dtype: float64\n",
      "HR                  1.0\n",
      "O2Sat               1.0\n",
      "Temp                1.0\n",
      "SBP                 1.0\n",
      "MAP                 1.0\n",
      "                   ... \n",
      "Resp_min_6h         1.0\n",
      "Resp_mean_6h        1.0\n",
      "Resp_median_6h      1.0\n",
      "Resp_std_6h         1.0\n",
      "Resp_diff_std_6h    1.0\n",
      "Length: 109, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the scaler is working ie the mean and std of the train set are 0 and 1\n",
    "print(train_df.mean())\n",
    "print(train_df.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train (no sample), test and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = f\"{project_root}/dataset/{OUTPUT_FOLDER_NAME}\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "val_df.to_parquet(f\"{output_folder}/val.parquet\")\n",
    "test_df.to_parquet(f\"{output_folder}/test.parquet\")\n",
    "train_df.to_parquet(f\"{output_folder}/no_sampling_train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the sampled datasets: one for undersampling and one for oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_train_df = over_under_sample(\n",
    "    df=train_df.copy(), method=\"oversample\", minority_ratio=SAMPLING_MINORITY_RATIO\n",
    ")\n",
    "undersampled_train_df = over_under_sample(\n",
    "    df=train_df.copy(), method=\"undersample\", minority_ratio=SAMPLING_MINORITY_RATIO\n",
    ")\n",
    "\n",
    "#Save the sampled datasets\n",
    "upsampled_train_df.to_parquet(f\"{output_folder}/oversampled_train.parquet\")\n",
    "undersampled_train_df.to_parquet(f\"{output_folder}/undersampled_train.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the balance ratio for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Total Patients</th>\n",
       "      <th>Non-Sepsis Patients</th>\n",
       "      <th>Non-Sepsis %</th>\n",
       "      <th>Sepsis Patients</th>\n",
       "      <th>Sepsis %</th>\n",
       "      <th>Imbalance Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Sampling (Train)</td>\n",
       "      <td>28234</td>\n",
       "      <td>26181</td>\n",
       "      <td>92.73%</td>\n",
       "      <td>2053</td>\n",
       "      <td>7.27%</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Validation</td>\n",
       "      <td>4034</td>\n",
       "      <td>3741</td>\n",
       "      <td>92.74%</td>\n",
       "      <td>293</td>\n",
       "      <td>7.26%</td>\n",
       "      <td>12.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test</td>\n",
       "      <td>8068</td>\n",
       "      <td>7482</td>\n",
       "      <td>92.74%</td>\n",
       "      <td>586</td>\n",
       "      <td>7.26%</td>\n",
       "      <td>12.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oversampled (Train)</td>\n",
       "      <td>37395</td>\n",
       "      <td>26175</td>\n",
       "      <td>70.00%</td>\n",
       "      <td>11220</td>\n",
       "      <td>30.00%</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Undersampled (Train)</td>\n",
       "      <td>6843</td>\n",
       "      <td>4790</td>\n",
       "      <td>70.00%</td>\n",
       "      <td>2053</td>\n",
       "      <td>30.00%</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Dataset  Total Patients  Non-Sepsis Patients Non-Sepsis %  \\\n",
       "0   No Sampling (Train)           28234                26181       92.73%   \n",
       "1            Validation            4034                 3741       92.74%   \n",
       "2                  Test            8068                 7482       92.74%   \n",
       "3   Oversampled (Train)           37395                26175       70.00%   \n",
       "4  Undersampled (Train)            6843                 4790       70.00%   \n",
       "\n",
       "   Sepsis Patients Sepsis % Imbalance Ratio  \n",
       "0             2053    7.27%           12.75  \n",
       "1              293    7.26%           12.77  \n",
       "2              586    7.26%           12.77  \n",
       "3            11220   30.00%            2.33  \n",
       "4             2053   30.00%            2.33  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_dataset_stats(df, name):\n",
    "    total_patients = df[\"patient_id\"].nunique()\n",
    "    patient_labels = df.groupby(\"patient_id\")[\"SepsisLabel\"].max()\n",
    "\n",
    "    non_sepsis = (patient_labels == 0).sum()\n",
    "    sepsis = (patient_labels == 1).sum()\n",
    "\n",
    "    non_sepsis_pct = non_sepsis / total_patients * 100\n",
    "    sepsis_pct = sepsis / total_patients * 100\n",
    "\n",
    "    imbalance_ratio = non_sepsis / sepsis if sepsis > 0 else float(\"inf\")\n",
    "\n",
    "    return {\n",
    "        \"Dataset\": name,\n",
    "        \"Total Patients\": total_patients,\n",
    "        \"Non-Sepsis Patients\": non_sepsis,\n",
    "        \"Non-Sepsis %\": f\"{non_sepsis_pct:.2f}%\",\n",
    "        \"Sepsis Patients\": sepsis,\n",
    "        \"Sepsis %\": f\"{sepsis_pct:.2f}%\",\n",
    "        \"Imbalance Ratio\": f\"{imbalance_ratio:.2f}\",\n",
    "    }\n",
    "\n",
    "datasets = [\n",
    "    (train_df, \"No Sampling (Train)\"),\n",
    "    (val_df, \"Validation\"),\n",
    "    (test_df, \"Test\"),\n",
    "    (upsampled_train_df, \"Oversampled (Train)\"),\n",
    "    (undersampled_train_df, \"Undersampled (Train)\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for df, name in datasets:\n",
    "    results.append(get_dataset_stats(df, name))\n",
    "\n",
    "# Create and display the results table\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HR',\n",
      " 'O2Sat',\n",
      " 'Temp',\n",
      " 'SBP',\n",
      " 'MAP',\n",
      " 'DBP',\n",
      " 'Resp',\n",
      " 'EtCO2',\n",
      " 'BaseExcess',\n",
      " 'HCO3',\n",
      " 'FiO2',\n",
      " 'pH',\n",
      " 'PaCO2',\n",
      " 'SaO2',\n",
      " 'AST',\n",
      " 'BUN',\n",
      " 'Alkalinephos',\n",
      " 'Calcium',\n",
      " 'Chloride',\n",
      " 'Creatinine',\n",
      " 'Bilirubin_direct',\n",
      " 'Glucose',\n",
      " 'Lactate',\n",
      " 'Magnesium',\n",
      " 'Phosphate',\n",
      " 'Potassium',\n",
      " 'Bilirubin_total',\n",
      " 'TroponinI',\n",
      " 'Hct',\n",
      " 'Hgb',\n",
      " 'PTT',\n",
      " 'WBC',\n",
      " 'Fibrinogen',\n",
      " 'Platelets',\n",
      " 'Age',\n",
      " 'Gender',\n",
      " 'HospAdmTime',\n",
      " 'ICULOS',\n",
      " 'SepsisLabel',\n",
      " 'patient_id',\n",
      " 'SOFA_Creatinine',\n",
      " 'SOFA_Platelets',\n",
      " 'SOFA_Bilirubin_total',\n",
      " 'SOFA_SaO2_FiO2',\n",
      " 'SOFA_score',\n",
      " 'NEWS_HR_score',\n",
      " 'NEWS_Resp_score',\n",
      " 'NEWS_Temp_score',\n",
      " 'NEWS_SBP_score',\n",
      " 'NEWS_O2Sat_score',\n",
      " 'NEWS_FiO2_score',\n",
      " 'NEWS_score',\n",
      " 'qSOFA_Resp_score',\n",
      " 'qSOFA_SBP_score',\n",
      " 'qSOFA_score',\n",
      " 'Shock_Index',\n",
      " 'Bilirubin_Ratio',\n",
      " 'SOFA_score_mean_global',\n",
      " 'SOFA_score_median_global',\n",
      " 'SOFA_score_max_global',\n",
      " 'SOFA_score_last_global',\n",
      " 'NEWS_score_mean_global',\n",
      " 'NEWS_score_median_global',\n",
      " 'NEWS_score_max_global',\n",
      " 'NEWS_score_last_global',\n",
      " 'qSOFA_score_mean_global',\n",
      " 'qSOFA_score_median_global',\n",
      " 'qSOFA_score_max_global',\n",
      " 'qSOFA_score_last_global',\n",
      " 'HR_missing_count_global',\n",
      " 'HR_missing_interval_mean_global',\n",
      " 'O2Sat_missing_count_global',\n",
      " 'O2Sat_missing_interval_mean_global',\n",
      " 'SBP_missing_count_global',\n",
      " 'SBP_missing_interval_mean_global',\n",
      " 'MAP_missing_count_global',\n",
      " 'MAP_missing_interval_mean_global',\n",
      " 'Resp_missing_count_global',\n",
      " 'Resp_missing_interval_mean_global',\n",
      " 'HR_max_6h',\n",
      " 'HR_min_6h',\n",
      " 'HR_mean_6h',\n",
      " 'HR_median_6h',\n",
      " 'HR_std_6h',\n",
      " 'HR_diff_std_6h',\n",
      " 'O2Sat_max_6h',\n",
      " 'O2Sat_min_6h',\n",
      " 'O2Sat_mean_6h',\n",
      " 'O2Sat_median_6h',\n",
      " 'O2Sat_std_6h',\n",
      " 'O2Sat_diff_std_6h',\n",
      " 'SBP_max_6h',\n",
      " 'SBP_min_6h',\n",
      " 'SBP_mean_6h',\n",
      " 'SBP_median_6h',\n",
      " 'SBP_std_6h',\n",
      " 'SBP_diff_std_6h',\n",
      " 'MAP_max_6h',\n",
      " 'MAP_min_6h',\n",
      " 'MAP_mean_6h',\n",
      " 'MAP_median_6h',\n",
      " 'MAP_std_6h',\n",
      " 'MAP_diff_std_6h',\n",
      " 'Resp_max_6h',\n",
      " 'Resp_min_6h',\n",
      " 'Resp_mean_6h',\n",
      " 'Resp_median_6h',\n",
      " 'Resp_std_6h',\n",
      " 'Resp_diff_std_6h']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure there is no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m patient_ids = json.load(\u001b[38;5;28mopen\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/patient_ids.json\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# check that the patient ids in the datasets are the same as the patient ids in the files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m train_df[\u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m].isin(patient_ids[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m]).all()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m val_df[\u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m].isin(patient_ids[\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m]).all()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m test_df[\u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m].isin(patient_ids[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]).all()\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# load the patient ids from the file\n",
    "patient_ids = json.load(open(f\"{output_folder}/patient_ids.json\"))\n",
    "\n",
    "# check that the patient ids in the datasets are the same as the patient ids in the files\n",
    "assert train_df[\"patient_id\"].isin(patient_ids[\"train\"]).all()\n",
    "assert val_df[\"patient_id\"].isin(patient_ids[\"val\"]).all()\n",
    "assert test_df[\"patient_id\"].isin(patient_ids[\"test\"]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not train_df[\"patient_id\"].isin(val_df[\"patient_id\"]).any()\n",
    "assert not train_df[\"patient_id\"].isin(test_df[\"patient_id\"]).any()\n",
    "assert not val_df[\"patient_id\"].isin(test_df[\"patient_id\"]).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # save the patient ids from the datasets to 1 json file with different keys\n",
    "# patient_ids = {\n",
    "#     \"train\": train_df[\"patient_id\"].unique().tolist(),\n",
    "#     \"val\": val_df[\"patient_id\"].unique().tolist(),\n",
    "#     \"test\": test_df[\"patient_id\"].unique().tolist()\n",
    "# }\n",
    "# with open(f\"{output_folder}/patient_ids.json\", \"w\") as f:\n",
    "#     json.dump(patient_ids, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
